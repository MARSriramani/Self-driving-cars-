{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **SELF-DRIVING CAR USING UDACITY’S CAR SIMULATOR ENVIRONMENT AND TRAINED BY DEEP NEURAL NETWORKS COMPLETE GUIDE**","metadata":{}},{"cell_type":"markdown","source":"We will be using Google Colab for doing the training process or Kaggle. We will open a new python3 notebook and get started. Next, we will git clone the repo.\n\n```!git clone https://github.com/Asikpalysik/Self-Driving-Car.git```","metadata":{}},{"cell_type":"markdown","source":"**The Training Process**\n\nWe will now import all the libraries needed for training process. It will use Tensorflow backend and keras at frontend.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom imgaug import augmenters as iaa\nimport cv2\nimport pandas as pd\nimport ntpath\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:53.079300Z","iopub.execute_input":"2022-02-03T10:01:53.079601Z","iopub.status.idle":"2022-02-03T10:01:53.087583Z","shell.execute_reply.started":"2022-02-03T10:01:53.079568Z","shell.execute_reply":"2022-02-03T10:01:53.086610Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"We wil use datadir as the name given to the folder itself and take the parameters itself. Using head, we will show the first five values for the CSV on the desired format.","metadata":{}},{"cell_type":"code","source":"datadir = ''\ncolumns = ['center', 'left', 'right', 'steering', 'throttle', 'reverse', 'speed']\ndata = pd.read_csv(os.path.join(datadir, '../input/self-driving-carbehavioural-cloning/driving_log.csv'), names = columns)\npd.set_option('display.max_colwidth', -1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:53.088982Z","iopub.execute_input":"2022-02-03T10:01:53.090078Z","iopub.status.idle":"2022-02-03T10:01:53.126642Z","shell.execute_reply.started":"2022-02-03T10:01:53.090036Z","shell.execute_reply":"2022-02-03T10:01:53.125783Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"As this is picking up the entire path from the local machine, we need to use ntpath function to get the network path assigned. We will declare a name path_leaf and assign accordingly.","metadata":{}},{"cell_type":"code","source":"def path_leaf(path):\n  head, tail = ntpath.split(path)\n  return tail\ndata['center'] = data['center'].apply(path_leaf)\ndata['left'] = data['left'].apply(path_leaf)\ndata['right'] = data['right'].apply(path_leaf)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:53.128602Z","iopub.execute_input":"2022-02-03T10:01:53.128984Z","iopub.status.idle":"2022-02-03T10:01:53.199339Z","shell.execute_reply.started":"2022-02-03T10:01:53.128943Z","shell.execute_reply":"2022-02-03T10:01:53.198407Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"We will bin the number of values where the number will be equal to 25 (odd number aimed to get center distribution). We will see the histogram using the np.histogram option on data frame ‘steering’, we will divide it to the number of bins.\nWe keep samples at 400 and then we draw a line. We see the data is centered along the middle that is 0.","metadata":{}},{"cell_type":"code","source":"num_bins = 25\nsamples_per_bin = 400\nhist, bins = np.histogram(data['steering'], num_bins)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:53.201124Z","iopub.execute_input":"2022-02-03T10:01:53.201448Z","iopub.status.idle":"2022-02-03T10:01:53.209075Z","shell.execute_reply.started":"2022-02-03T10:01:53.201406Z","shell.execute_reply":"2022-02-03T10:01:53.207982Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"print(bins)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:53.211535Z","iopub.execute_input":"2022-02-03T10:01:53.212082Z","iopub.status.idle":"2022-02-03T10:01:53.221160Z","shell.execute_reply.started":"2022-02-03T10:01:53.212036Z","shell.execute_reply":"2022-02-03T10:01:53.220469Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"print(bins)","metadata":{}},{"cell_type":"code","source":"center = (bins[:-1]+ bins[1:]) * 0.5\nplt.bar(center, hist, width=0.05)\nplt.plot((np.min(data['steering']), np.max(data['steering'])), \\\n(samples_per_bin, samples_per_bin))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:53.222445Z","iopub.execute_input":"2022-02-03T10:01:53.222865Z","iopub.status.idle":"2022-02-03T10:01:53.500047Z","shell.execute_reply.started":"2022-02-03T10:01:53.222835Z","shell.execute_reply":"2022-02-03T10:01:53.499184Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"print('total data:', len(data))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:53.501194Z","iopub.execute_input":"2022-02-03T10:01:53.501424Z","iopub.status.idle":"2022-02-03T10:01:53.506743Z","shell.execute_reply.started":"2022-02-03T10:01:53.501397Z","shell.execute_reply":"2022-02-03T10:01:53.505957Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"We wil specify a variable remove_list.We will specify samples we want to remove using looping construct through every single bin we will iterate through all the steering data. We will shuffle the data and romve some from it as it is now uniformly structured after shuffling.The output will be the distribution of steering angle that are much more uniform. There are significant amount of left steering angle and right steering angle eliminating the bias to drive straight all the time.","metadata":{}},{"cell_type":"code","source":"remove_list = []\nfor j in range(num_bins):\n  list_ = []\n  for i in range(len(data['steering'])):\n    if data['steering'][i] >= bins[j] and data['steering'][i] <= bins[j+1]:\n      list_.append(i)\n  list_ = shuffle(list_)\n  list_ = list_[samples_per_bin:]\n  remove_list.extend(list_)\nprint('removed:', len(remove_list))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:53.507986Z","iopub.execute_input":"2022-02-03T10:01:53.508214Z","iopub.status.idle":"2022-02-03T10:01:54.073484Z","shell.execute_reply.started":"2022-02-03T10:01:53.508187Z","shell.execute_reply":"2022-02-03T10:01:54.072558Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"data.drop(data.index[remove_list], inplace=True)\nprint('remaining:', len(data))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.074733Z","iopub.execute_input":"2022-02-03T10:01:54.075000Z","iopub.status.idle":"2022-02-03T10:01:54.081856Z","shell.execute_reply.started":"2022-02-03T10:01:54.074969Z","shell.execute_reply":"2022-02-03T10:01:54.081185Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"Plot on it","metadata":{}},{"cell_type":"code","source":"hist, _ = np.histogram(data['steering'], (num_bins))\nplt.bar(center, hist, width=0.05)\nplt.plot((np.min(data['steering']), np.max(data['steering'])), \\\n(samples_per_bin, samples_per_bin))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.082862Z","iopub.execute_input":"2022-02-03T10:01:54.083163Z","iopub.status.idle":"2022-02-03T10:01:54.366666Z","shell.execute_reply.started":"2022-02-03T10:01:54.083132Z","shell.execute_reply":"2022-02-03T10:01:54.365743Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"print(data.iloc[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.369306Z","iopub.execute_input":"2022-02-03T10:01:54.369537Z","iopub.status.idle":"2022-02-03T10:01:54.375829Z","shell.execute_reply.started":"2022-02-03T10:01:54.369510Z","shell.execute_reply":"2022-02-03T10:01:54.374881Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"We will now load the image into array to manipulate them accordingly. We will define a function named locd_img_steering. We will have image path as empty list and steering as empty list and then loop through. We use iloc selector as data frame based on the specific index we will use cut data for now.","metadata":{}},{"cell_type":"code","source":"def load_img_steering(datadir, df):\n  image_path = []\n  steering = []\n  for i in range(len(data)):\n    indexed_data = data.iloc[i]\n    center, left, right = indexed_data[0], indexed_data[1], indexed_data[2]\n    image_path.append(os.path.join(datadir, center.strip()))\n    steering.append(float(indexed_data[3]))\n    # left image append\n    image_path.append(os.path.join(datadir,left.strip()))\n    steering.append(float(indexed_data[3])+0.15)\n    # right image append\n    image_path.append(os.path.join(datadir,right.strip()))\n    steering.append(float(indexed_data[3])-0.15)\n  image_paths = np.asarray(image_path)\n  steerings = np.asarray(steering)\n  return image_paths, steerings\n \nimage_paths, steerings = load_img_steering(datadir + '../input/self-driving-carbehavioural-cloning/IMG', data)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.376994Z","iopub.execute_input":"2022-02-03T10:01:54.377214Z","iopub.status.idle":"2022-02-03T10:01:54.509739Z","shell.execute_reply.started":"2022-02-03T10:01:54.377187Z","shell.execute_reply":"2022-02-03T10:01:54.508881Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"We will be splitting the image path as well as storing arrays accordingly.","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(image_paths, steerings, test_size=0.2, random_state=6)\nprint('Training Samples: {}\\nValid Samples: {}'.format(len(X_train), len(X_valid)))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.511031Z","iopub.execute_input":"2022-02-03T10:01:54.511397Z","iopub.status.idle":"2022-02-03T10:01:54.519599Z","shell.execute_reply.started":"2022-02-03T10:01:54.511311Z","shell.execute_reply":"2022-02-03T10:01:54.518657Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"We will have the histograms now.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].hist(y_train, bins=num_bins, width=0.05, color='blue')\naxes[0].set_title('Training set')\naxes[1].hist(y_valid, bins=num_bins, width=0.05, color='red')\naxes[1].set_title('Validation set')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.521116Z","iopub.execute_input":"2022-02-03T10:01:54.521772Z","iopub.status.idle":"2022-02-03T10:01:54.942580Z","shell.execute_reply.started":"2022-02-03T10:01:54.521730Z","shell.execute_reply":"2022-02-03T10:01:54.941740Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"**Augmentation and image pre-processing**\n\nThe biggest challenge was generalizing the behavior of the car on Track_2 which it was never trained for. In a real-life situation, we can never train a self-driving car model for every track possible, as the data will be too huge to process. Also, it is not possible to gather the dataset for all the weather conditions and roads. Thus, there is a need to come up with an idea of generalizing the behavior on different tracks. This problem is solved using image preprocessing and augmentation techniques.","metadata":{}},{"cell_type":"markdown","source":"*Zoom*\n\nThe images in the dataset have relevant features in the lower part where the road is visible. The external environment above a certain image portion will never be used to determine the output and thus can be cropped. Approximately, 30% of the top portion of the image is cut and passed in the training set. The snippet of code and transformation of an image after cropping and resizing it to original image can be seen in below.","metadata":{}},{"cell_type":"code","source":"def zoom(image):\n  zoom = iaa.Affine(scale=(1, 1.3))\n  image = zoom.augment_image(image)\n  return image\nimage = image_paths[random.randint(0, 1000)]\noriginal_image = mpimg.imread(image)\nzoomed_image = zoom(original_image)\n \nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n \naxs[0].imshow(original_image)\naxs[0].set_title('Original Image')\n \naxs[1].imshow(zoomed_image)\naxs[1].set_title('Zoomed Image')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.943867Z","iopub.execute_input":"2022-02-03T10:01:54.944497Z","iopub.status.idle":"2022-02-03T10:01:54.979546Z","shell.execute_reply.started":"2022-02-03T10:01:54.944465Z","shell.execute_reply":"2022-02-03T10:01:54.978721Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"*Shift (horizontal/vertical)*\n\nThe image is shifted by a small amount, it is vertical shift and horizontal shift as below.\n","metadata":{}},{"cell_type":"code","source":"def pan(image):\n  pan = iaa.Affine(translate_percent= {\"x\" : (-0.1, 0.1), \"y\": (-0.1, 0.1)})\n  image = pan.augment_image(image)\n  return image\nimage = image_paths[random.randint(0, 1000)]\noriginal_image = mpimg.imread(image)\npanned_image = pan(original_image)\n \nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n \naxs[0].imshow(original_image)\naxs[0].set_title('Original Image')\n \naxs[1].imshow(panned_image)\naxs[1].set_title('Panned Image')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.980339Z","iopub.status.idle":"2022-02-03T10:01:54.981232Z","shell.execute_reply.started":"2022-02-03T10:01:54.981024Z","shell.execute_reply":"2022-02-03T10:01:54.981047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Brightness*\n\nTo generalize to the weather conditions with bright sunny day or cloudy, lowlight conditions, the brightness augmentation can prove to be very useful. The code snippet and increase of brightness can be seen below. Similarly, I have randomly also lowered down the level of brightness for other conditions. \n","metadata":{}},{"cell_type":"code","source":"def img_random_brightness(image):\n    brightness = iaa.Multiply((0.2, 1.2))\n    image = brightness.augment_image(image)\n    return image\nimage = image_paths[random.randint(0, 1000)]\noriginal_image = mpimg.imread(image)\nbrightness_altered_image = img_random_brightness(original_image)\n \nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n \naxs[0].imshow(original_image)\naxs[0].set_title('Original Image')\n \naxs[1].imshow(brightness_altered_image)\naxs[1].set_title('Brightness altered image ')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.982034Z","iopub.status.idle":"2022-02-03T10:01:54.982693Z","shell.execute_reply.started":"2022-02-03T10:01:54.982494Z","shell.execute_reply":"2022-02-03T10:01:54.982515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Flip (horizontal)*\n\nThe image is flipped horizontally (i.e. a mirror image of the original image is passed to the dataset). The motive behind this is that the model gets trained for similar kinds of turns on opposite sides too. This is important because Track 1 includes only left turns. The snippet of code and transformation of an image after flipping it can be seen in below.","metadata":{}},{"cell_type":"code","source":"def img_random_flip(image, steering_angle):\n    image = cv2.flip(image,1)\n    steering_angle = -steering_angle\n    return image, steering_angle\nrandom_index = random.randint(0, 1000)\nimage = image_paths[random_index]\nsteering_angle = steerings[random_index]\n \n \noriginal_image = mpimg.imread(image)\nflipped_image, flipped_steering_angle = img_random_flip(original_image, steering_angle)\n \nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n \naxs[0].imshow(original_image)\naxs[0].set_title('Original Image - ' + 'Steering Angle:' + str(steering_angle))\n \naxs[1].imshow(flipped_image)\naxs[1].set_title('Flipped Image - ' + 'Steering Angle:' + str(flipped_steering_angle))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.983479Z","iopub.status.idle":"2022-02-03T10:01:54.983760Z","shell.execute_reply.started":"2022-02-03T10:01:54.983610Z","shell.execute_reply":"2022-02-03T10:01:54.983625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To have a look what we have at this moment","metadata":{}},{"cell_type":"code","source":"def random_augment(image, steering_angle):\n    image = mpimg.imread(image)\n    if np.random.rand() < 0.5:\n      image = pan(image)\n    if np.random.rand() < 0.5:\n      image = zoom(image)\n    if np.random.rand() < 0.5:\n      image = img_random_brightness(image)\n    if np.random.rand() < 0.5:\n      image, steering_angle = img_random_flip(image, steering_angle)\n    \n    return image, steering_angle\nncol = 2\nnrow = 10\n \nfig, axs = plt.subplots(nrow, ncol, figsize=(15, 50))\nfig.tight_layout()\n \nfor i in range(10):\n  randnum = random.randint(0, len(image_paths) - 1)\n  random_image = image_paths[randnum]\n  random_steering = steerings[randnum]\n    \n  original_image = mpimg.imread(random_image)\n  augmented_image, steering = random_augment(random_image, random_steering)\n    \n  axs[i][0].imshow(original_image)\n  axs[i][0].set_title(\"Original Image\")\n  \n  axs[i][1].imshow(augmented_image)\n  axs[i][1].set_title(\"Augmented Image\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.984814Z","iopub.status.idle":"2022-02-03T10:01:54.985408Z","shell.execute_reply.started":"2022-02-03T10:01:54.985026Z","shell.execute_reply":"2022-02-03T10:01:54.985135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I continued by doing some image processing. I cropped the image to remove the unnecessary features, changes the images to YUV format, used gaussian blur, decreased the size for easier processing and normalized the values","metadata":{}},{"cell_type":"code","source":"def img_preprocess(img):\n    img = img[60:135,:,:]\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n    img = cv2.GaussianBlur(img,  (3, 3), 0)\n    img = cv2.resize(img, (200, 66))\n    img = img/255\n    return img\nimage = image_paths[100]\noriginal_image = mpimg.imread(image)\npreprocessed_image = img_preprocess(original_image)\n \nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\naxs[0].imshow(original_image)\naxs[0].set_title('Original Image')\naxs[1].imshow(preprocessed_image)\naxs[1].set_title('Preprocessed Image')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.986431Z","iopub.status.idle":"2022-02-03T10:01:54.986732Z","shell.execute_reply.started":"2022-02-03T10:01:54.986576Z","shell.execute_reply":"2022-02-03T10:01:54.986592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To compare and visualize I plotted the original and the pre-processed image.","metadata":{}},{"cell_type":"code","source":"image = image_paths[100]\noriginal_image = mpimg.imread(image)\npreprocessed_image = img_preprocess(original_image)\n \nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\naxs[0].imshow(original_image)\naxs[0].set_title('Original Image')\naxs[1].imshow(preprocessed_image)\naxs[1].set_title('Preprocessed Image')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.987662Z","iopub.status.idle":"2022-02-03T10:01:54.987966Z","shell.execute_reply.started":"2022-02-03T10:01:54.987796Z","shell.execute_reply":"2022-02-03T10:01:54.987812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batch_generator(image_paths, steering_ang, batch_size, istraining):\n  \n  while True:\n    batch_img = []\n    batch_steering = []\n    \n    for i in range(batch_size):\n      random_index = random.randint(0, len(image_paths) - 1)\n      \n      if istraining:\n        im, steering = random_augment(image_paths[random_index], steering_ang[random_index])\n     \n      else:\n        im = mpimg.imread(image_paths[random_index])\n        steering = steering_ang[random_index]\n      \n      im = img_preprocess(im)\n      batch_img.append(im)\n      batch_steering.append(steering)\n    yield (np.asarray(batch_img), np.asarray(batch_steering)) ","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.989000Z","iopub.status.idle":"2022-02-03T10:01:54.989278Z","shell.execute_reply.started":"2022-02-03T10:01:54.989129Z","shell.execute_reply":"2022-02-03T10:01:54.989144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So far so good. Next, I converted all the images into numpy array.","metadata":{}},{"cell_type":"code","source":"x_train_gen, y_train_gen = next(batch_generator(X_train, y_train, 1, 1))\nx_valid_gen, y_valid_gen = next(batch_generator(X_valid, y_valid, 1, 0))\n \nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.tight_layout()\n \naxs[0].imshow(x_train_gen[0])\naxs[0].set_title('Training Image')\n \naxs[1].imshow(x_valid_gen[0])\naxs[1].set_title('Validation Image')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.990165Z","iopub.status.idle":"2022-02-03T10:01:54.990484Z","shell.execute_reply.started":"2022-02-03T10:01:54.990318Z","shell.execute_reply":"2022-02-03T10:01:54.990347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will design our Model architecture. We have to classify the traffic signs too that's why we need to shift from Lenet 5 model to NVDIA model. With behavioural cloning, our dataset is much more complex then any dataset we have used.\nWe are dealing with images that have (200,66) dimensions.\nOur current datset has 5386 images to train with but MNSIT has around 60,000 images to train with.\nOur behavioural cloning code has simply has to return appropriate steering angle which is a regression type example.\nFor these things, we need a more advanced model which is provided by nvdia and known as nvdia model.The architecture of nvdia model is as shown below:\n![](http://miro.medium.com/max/4800/1*2Z_8DB1ybUmRaHUsyi6bSA.png)","metadata":{}},{"cell_type":"markdown","source":"For defining the model architecture, we need to define the model object.\nNormalization state can be skipped as we have already normalized it.\nWe will add the convolution layer.\nAs compared to the model, we will organize accordingly.\nThe Nvdia model uses 24 filters in the layer along with a kernel of size 5,5.\nWe will introduce sub sampling. The function reflects to stride length of the kernel as it processes through an image, we have large images.\nHorizontal movement with 2 pixels at a time, similarly vertical movement to 2 pixels at a time.\nAs this is the first layer, we have to define input shape of the model too i.e., (66,200,3) and the last function is an activation function that is “elu”.","metadata":{}},{"cell_type":"markdown","source":"Revisting the model, we see that our second layer has 36 filters with kernel size (5,5) same subsampling option with stride length of (2,2) and conclude this layer with activation ‘elu’.","metadata":{}},{"cell_type":"markdown","source":"According to Nvdia model, it shows we have 3 more layers in the convolutional neural network. With 48 filters, with 64 filters (3,3) kernel 64 filters (3,3) kernel Dimensions have been reduced significantly so for that we will remove subsampling from 4th and 5th layer.","metadata":{}},{"cell_type":"markdown","source":"Next we add a flatten layer. We will take the output array from previous convolution neural network to convert it into a one dimensional array so that it can be fed to fully connected layer to follow.","metadata":{}},{"cell_type":"markdown","source":"Our last convolution layer outputs an array shape of (1,18) by 64.","metadata":{}},{"cell_type":"markdown","source":"We end the architecture of Nvdia model with a dense layer containing a single output node which will output the predicted steering angle for our self driving car. Now we will use model.compile() to compile our architecture as this is a regression type example the metrics that we will be using will be mean squared error and optimize as Adam. We will be using relatively a low learning rate that it can help on accuracy. We will use dropout layer to avoid overfitting the data. Dropout Layer sets the input of random fraction of nodes to “0” during each update. During this, we will generate the training data as it is forced to use a variety of combination of nodes to learn from the same data. We will have to separate the convolution layer from fully connected layer with a factor of 0.5 is added so it converts 50 percent of the input to 0. We Will define the model by calling the nvdia model itself. Now we will have the model training process.To define training parameters, we will use model.fit(), we will import our training data X_Train, training data ->y_train, we have less data on the datasets we will require more epochs to be effective. We will use validation data and then use Batch size.","metadata":{}},{"cell_type":"code","source":"def nvidiaModel():\n  model = Sequential()\n  model.add(Convolution2D(24,(5,5),strides=(2,2),input_shape=(66,200,3),activation=\"elu\"))\n  model.add(Convolution2D(36,(5,5),strides=(2,2),activation=\"elu\"))\n  model.add(Convolution2D(48,(5,5),strides=(2,2),activation=\"elu\")) \n  model.add(Convolution2D(64,(3,3),activation=\"elu\"))   \n  model.add(Convolution2D(64,(3,3),activation=\"elu\"))\n  model.add(Dropout(0.5))\n  \n  model.add(Flatten())\n  \n  model.add(Dense(100,activation=\"elu\"))\n  model.add(Dropout(0.5))\n  \n  model.add(Dense(50,activation=\"elu\"))\n  model.add(Dropout(0.5))\n  \n  model.add(Dense(10,activation=\"elu\"))\n  model.add(Dropout(0.5))\n  \n  model.add(Dense(1))\n  model.compile(optimizer=Adam(lr=1e-3),loss=\"mse\")\n  \n  return model","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.993388Z","iopub.status.idle":"2022-02-03T10:01:54.994075Z","shell.execute_reply.started":"2022-02-03T10:01:54.993736Z","shell.execute_reply":"2022-02-03T10:01:54.993765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = nvidiaModel()\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.995485Z","iopub.status.idle":"2022-02-03T10:01:54.996145Z","shell.execute_reply.started":"2022-02-03T10:01:54.995823Z","shell.execute_reply":"2022-02-03T10:01:54.995871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Results**\n\nThe following results were observed for described architectures. I had to come up with two different performance metrics.\n- Value loss or Accuracy (computed during training phase)\n- Generalization on Track 1 (drive performance)\n","metadata":{}},{"cell_type":"markdown","source":"**Value loss or Accuracy**\n\nThe first evaluation parameter considered here is “Loss” over each epoch of the training run. To calculate value loss over each epoch, Keras provides “val_loss”, which is the average loss after that epoch. The loss observed during the initial epochs at the beginning of training phase is high, but it falls gradually, and that is evident by the screenshots below which shows the run of Architecture in the training phase.","metadata":{}},{"cell_type":"code","source":"history = model.fit_generator(batch_generator(X_train, y_train, 100, 1),\n                                  steps_per_epoch=300, \n                                  epochs=10,\n                                  validation_data=batch_generator(X_valid, y_valid, 100, 0),\n                                  validation_steps=200,\n                                  verbose=1,\n                                  shuffle = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.997606Z","iopub.status.idle":"2022-02-03T10:01:54.998217Z","shell.execute_reply.started":"2022-02-03T10:01:54.997948Z","shell.execute_reply":"2022-02-03T10:01:54.997976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Why We Use ELU Over RELU**\n\nWe can have dead relu this is when a node in neural network essentially dies and only feeds a value of zero to nodes which follows it. We will change from relu to elu. Elu function has always a chance to recover and fix it errors means it is in a process of learning and contributing to the model. We will plot the model and then save it accordingly in h5 format for a keras file.","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['training', 'validation'])\nplt.title('Loss')\nplt.xlabel('Epoch')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:54.999569Z","iopub.status.idle":"2022-02-03T10:01:55.000064Z","shell.execute_reply.started":"2022-02-03T10:01:54.999783Z","shell.execute_reply":"2022-02-03T10:01:54.999807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will save the model.","metadata":{}},{"cell_type":"code","source":"model.save('model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T10:01:55.001400Z","iopub.status.idle":"2022-02-03T10:01:55.001854Z","shell.execute_reply.started":"2022-02-03T10:01:55.001605Z","shell.execute_reply":"2022-02-03T10:01:55.001630Z"},"trusted":true},"execution_count":null,"outputs":[]}]}